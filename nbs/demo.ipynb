{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/workspace/.cache/huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/projects/selfie/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/workspace/projects/selfie/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/workspace/projects/selfie/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/workspace/projects/selfie/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/workspace/projects/selfie/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 15/15 [06:36<00:00, 26.44s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 15/15 [00:55<00:00,  3.70s/it]\n",
      "/workspace/projects/selfie/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model_path = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,\n",
    "#     llm_int8_threshold=6.0,\n",
    "#     llm_int8_has_fp16_weight=False,\n",
    "#     llm_int8_enable_fp32_cpu_offload=False\n",
    "# )\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,  # or your specific Llama 70B variant\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device, \n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/projects/selfie/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from selfie.interpret import InterpretationPrompt, interpret\n",
    "interpretation_prompt = InterpretationPrompt(tokenizer, (\"[INST]\", 0, 0, 0, 0, 0, \"[/INST] Sure, I will summarize the message:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in model: 80\n"
     ]
    }
   ],
   "source": [
    "num_layers = model.config.num_hidden_layers\n",
    "print(f\"Number of layers in model: {num_layers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ▁[\n",
      "1 INST\n",
      "2 ]\n",
      "3 ▁What\n",
      "4 '\n",
      "5 s\n",
      "6 ▁highest\n",
      "7 ▁mountain\n",
      "8 ▁in\n",
      "9 ▁the\n",
      "10 ▁world\n",
      "11 ?\n",
      "12 ▁[\n",
      "13 /\n",
      "14 INST\n",
      "15 ]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"[INST] What's highest mountain in the world? [/INST]\")\n",
    "for i, token in enumerate(tokens):\n",
    "    print(i, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting '[INST] What's highest mountain in the world? [/INST]' with '[INST]_ _ _ _ _ [/INST] Sure, I will summarize the message:'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/workspace/projects/selfie/selfie/generate_wrappers.py:733: UserWarning: You are calling transformers.generation.generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.36s/it]\n"
     ]
    }
   ],
   "source": [
    "original_prompt = \"[INST] What's highest mountain in the world? [/INST]\"\n",
    "tokens_to_interpret = [(40, 8), (60, 8)]\n",
    "bs = 2\n",
    "max_new_tokens = 20\n",
    "k = 1\n",
    "\n",
    "interpretation_df = interpret(original_prompt=original_prompt, tokens_to_interpret=tokens_to_interpret, model=model, interpretation_prompt=interpretation_prompt, bs=bs, max_new_tokens=max_new_tokens, k=k, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>interpretation</th>\n",
       "      <th>layer</th>\n",
       "      <th>token</th>\n",
       "      <th>token_decoded</th>\n",
       "      <th>relevancy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[INST] What's highest mountain in the world? [...</td>\n",
       "      <td>\\n\\nThe message is: _ _ _ _\\n\\nPlease provide ...</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>mountain</td>\n",
       "      <td>[0.1953, 0.1831, 0.04224, 0.8706, 0.413, 0.510...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[INST] What's highest mountain in the world? [...</td>\n",
       "      <td>\\n\\n\"I don't know what to do. I'm feeling real...</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "      <td>mountain</td>\n",
       "      <td>[0.0874, 0.253, 0.2974, 0.5884, 0.6553, 0.0014...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  [INST] What's highest mountain in the world? [...   \n",
       "1  [INST] What's highest mountain in the world? [...   \n",
       "\n",
       "                                      interpretation  layer  token  \\\n",
       "0  \\n\\nThe message is: _ _ _ _\\n\\nPlease provide ...     40      8   \n",
       "1  \\n\\n\"I don't know what to do. I'm feeling real...     60      8   \n",
       "\n",
       "  token_decoded                                    relevancy_score  \n",
       "0      mountain  [0.1953, 0.1831, 0.04224, 0.8706, 0.413, 0.510...  \n",
       "1      mountain  [0.0874, 0.253, 0.2974, 0.5884, 0.6553, 0.0014...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(interpretation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"I don't know what to do. I'm feeling really down and hop\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(interpretation_df)['interpretation'][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
